CLIP+SAM 조합으로 이미지 내에서 사전 등록된 텍스트(프롬프트) 부합하는 영역(마스크)에 붉은 표시를 하는 파이프라인

[동작 과정]
1. 이미지 로드						-> 이 부분을 카메라에서 받은 비디오, 한 프레임의 이미지 등을 받게 구성 필요
2. SAM으로 전체 마스크 분할
3. 각 마스크마다 이미지 crop
4. CLIP으로 crop 이미지와 텍스트 비교
5. CLIP score가 임계값 이상인 마스크 선택
6. 선택된 마스크를 원본 이미지 위에 시각화	-> 이 부분을 햅틱 피드백, 음성 출력으로 구성 필요

+ 2~5에서 전처리 최적화, 모델 경량화 필요


#특기사항
현재 데스크탑으로 수행하기 때문에 GPU 사용 기반 -> 'Device: cuda'
데스크탑 기준 2~3초 내로 VLM이 수행되고 결과가 출력되나, 사용 보드 성능 따라 시간 엄청 늘어날수도 있음
노트북, 코랩, Device:CUDA 조합 기준 20~40초 걸림 -> 연산 하드웨어 따라 성능 많이 갈릴 것 같음
#코랩 말고, 파이썬 코드를 노트북 자체로 돌려보기


#문제점
threshold 


#대략 다음 테스크
일단 보드, 라이다 정해지지 않은 시점인 만큼
1. 비디오에서 순간 이미지 추출, 그 이미지에서 CLIP+SAM 수행 방법론 찾아보고 적용
2. 목적에 따라 전처리과정 최적화
3. 동작에 시간 꽤 걸릴 수 있음 감안하고, VLM 사용결정
4. windows -> linux 변경 방법 확인해두기


5. 보드, 카메라 등 주요 하드웨어 선택하고 주문ㄱㄱ
6. 가능하면 사용 카메라를 먼저 컴퓨터 연결해서 실시간으로 영상 받고, VLM 돌리는 코드 구성, 시험




#최적화 방향
SAM 모델을 vit_h 대신 vit_t (tiny)로 교체 (속도 수배 개선)

CLIP도 ViT-B/32 → RN50, MobileCLIP 등 경량화 버전 고려

마스크 수 제한 (전체 마스크 중 상위 N개만)

cv2/PIL 대신 Tensor 연산 전환 등


#최적화 포인트
1. SAM 마스크 생성
    가장 연산량 많은 부분. 해상도 줄이기 OR ROI 기반 crop 후 SAM 수행 방법 OR points 지정해주는 인터랙티브 방법

2. CLIP 이미지 전처리
  2.1 imgs = torch.stack([preprocess(im).to(device) for im in images])
    preprocess()는 이미지 크기 조절, 정규화 등의 기능
    PIL 이미지 처리보다 OpenCV 기반 전처리 대체 시 속도 개선
    미리 텐서 크기로 전환하고 배치로 처리하는게 속도 이점
  2.2 그래도 많이 느릴 경우 CLIP 모델을 Vit-B/32보다 더 경량 모델 (RN50 등) 수정

CLIP 임베딩을 저장해놓고 재사용(캐시화)

-> 적용 방법 얘기해보고 적용, 실제 학교 근처 거리 사진으로 시험
